---
title: AB测试的例子：访问时长
date: 2019-06-16 11:46:52
categories:
  - 增长黑客
tags:
  - R
---

{% asset_img abtest.jpeg "题图" %}

上一篇关于AB测试的文章里，我们模拟了一个常见的 ab 测试场景：转化率。这个场景适用于计算百分比数据的场景。

还有一种场景，数据不是百分比，而是连续数值。例如：
- 访问时长
- 人均消费
- 人均阅读数量
- ...

这个问题场景下，不适合使用 r 的 `prop.test` 函数。我们可以使用 z-test。

<!-- more -->

## 场景

我们是一个内容 app，产品部门开发了一个新版本，我们想要测试新版本下的用户访问时长是否会提高。

我们的原假设和备择假设是：
- H0：新方案的访问时长 <= 老方案的访问时长
- Ha：新方案的访问时长 > 老方案的访问时长

我们选择单侧检验有2个原因：
- 从产品迭代流程考虑，如果没有证据正面新方案比老方案好，我们就不会全面部署新方案，所以我们寻找证据“证伪”老方案
- 从统计功效考虑，使用单侧检验能在相同样本下保证更高的统计功效，也就是节约成本了

因为比较的是两个总体的分布是否有差距，我们使用 two sample t-test。

## 参数设定和样本数量估计

跟之前一样，我们设定 80% 的 power 和 0.05 的 significance level。

t-test 的样本量估算需要先给定一个 cohen'd。

d = (m1 - m2 ) /  delta，其中：
- m1 和 m2 分别是两组的均值
- delta 是 common standard deviation

在开始测试之前，我们只知道 m2 的时长，假设 m2 = 60。理论上我们不可能知道 delta，但我们可以用老方案的样本去估算。一个“便利公式”是 delta = （最大值 - 最小值）/4。假定 delta = 25。

现在，我们要判断：我们预期新方案的均值有多大的提升呢？假设我们期望新方案会比老方案提高5，也就是从60提高到65。这时我们可以计算 cohen'd = （65 - 60） / 25 = 0.2

这时我们可以用 R 的 pwr package 估算需要的样本量：

{% codeblock lang:r %}
pwr.t.test(d = 0.2, sig.level = 0.05, power = 0.8, type = 'two.sample', alternative='greater')
{% endcodeblock %}

这个计算告诉我们，每个组需要310个样本。我们着手安排测试。


## 检查数据

一天之后，我们搜集够了650个样本，每组325个样本，我们开始检查数据。

我们发现两个组的数据如下：
- 新方案：平均时长64秒，标准差43秒
- 原方案：平均时长60秒，标准差40秒

看上去新方案更好，但我们能得出新方案更好的结论吗？

我们可以用 `t.test` 做检验：

{% codeblock lang:r %}
x <- rnorm(315, 64, 43)
y <- rnorm(315, 60 ,40)

t.test(x, y , alternative = 'greater')
{% endcodeblock %}

我在这里模拟了数据，实际上我们可以直接带入两个 list 进 t.test，或者用现成的能输入均值和标准差的函数。

结果：p值 = 0.21，我们无法拒绝原假设。

结论：新方案未必比老方案好，我们需要做进一步测试（增加样本量），或保留老方案。


另一个平行宇宙里，新方案的平均时长还是64秒，标准差是40秒，我们的结论又会变成怎样呢？

{% codeblock lang:r %}
x <- rnorm(315, 64, 40)
y <- rnorm(315, 60 ,40)

t.test(x, y , alternative = 'greater')
{% endcodeblock %}

p值 = 0.04956，小于 0.05。
结论：我们拒绝原假设，认为新方案更好。

## AB 测试不只关注均值，还关注分布

我经常看到不了解统计学的同事用两个均值作为方案优劣的比较依据。上面的例子可以说明：均值差异是不够的。很多时候，我们观察到的均值差异只是随机现象的结果。

科学的方法还会考虑分布，怎么考虑分布呢？选择一个合适的 test 计算统计显著性。

不是每个人都懂统计学，但有的人会懂得尊重科学。

当我们从公司层面追求科学的数据方法论，公司从上到下的决策水平都会提高。
